---
title: "Time Series Forecasting Asian Paints"
output:
  html_notebook:
    css: styles.css
---

### 1. Initial Setup

#### 1.1. Clearing Previously Loaded Objects from Memory

First, I wanted a clean slate to start running our code, and that is why I removed all currently loaded objects from R environment.\
ls() function gives a list of current objects loaded in r environment, and rm() function removes given object from memory.

```{r}
rm(list=ls())
```

#### 1.2. Installing and Loading Packages

Now, we are installing and loading all the required packages to do our analysis, plotting graphs, and creating models.\
"install.packages()" installs all the packages required, and then lapply() function loads them one by one.

```{r}
# Required Packages
packages = c('quantmod','tseries', 'forecast','FinTS', 'rugarch', 'dplyr', 'ggplot2', 'zoo', 'lmtest', 'knitr', 'kableExtra')

# Installing the Required Packages
# install.packages(packages, dependencies = TRUE)

# Loading the Required Packages
lapply(packages, require, character.only = TRUE)
```

### 2. Data Preparation

#### 2.1. Fetching Asian Paints Daily Stock Price Data of Last 10 Years

I've chosen Asian Paints Daily Stock Price Data to do my time-series analysis. I went to yahoo finance and checked the symbols for the Asian Paints stock listed on the National Stock Exchange.\
Using the symbol of Asian Paints in getSymbols() function, I fetched the daily stock data for past 3 years. I've deliberately excluded the COVID-influenced time period i.e. between Indian FY-21 to FY-22 to prevent any misleading inferences out of the data.

```{r}
getSymbols(Symbols = 'ASIANPAINT.NS', 
           src = 'yahoo', 
           from = as.Date('2022-04-01'), 
           to = as.Date('2025-03-31'),
           periodicity = 'daily')
```

#### 2.2. Retrieving and Cleaning the Daily Adjusted Closing Price

I also created a simple data frame object of daily adjusted closing prices of the stock named **"ap_price"**, for plotting graphs and doing any further analysis.

```{r}
#Adjusted Closing Price
ap_price = na.omit(subset(data.frame(date=index(ASIANPAINT.NS), coredata(ASIANPAINT.NS)), select = c(date, ASIANPAINT.NS.Adjusted)))
class(ap_price)
```

```{r}
#getwd()
write.csv(ap_price, file = "ap_price.csv", row.names = FALSE)
```

### 3. Data Exploration

#### 3.1. Exploring Daily Closing Price of Asian Paints

To explore the data, I plotted a graph of adjusted closing stock prices using the plot() function.

```{r}

# Plot the line graph
plot(ap_price$date, ap_price$ASIANPAINT.NS.Adjusted,
     type = "l", col = "darkblue", lwd = 1.5,
     main = "Daily Adjusted Closing Price of Asian Paints",
     xlab = "Date", ylab = "Adjusted Closing Price",
     xaxt = "n", cex.main = 1.4, cex.lab = 0.9, cex.axis = 0.9)

# Add custom x-axis showing months
axis.Date(1, at = seq(min(ap_price$date), max(ap_price$date), by = "months"), format = "%m-%Y")
```

**Inference:** It seems from the first look that it has some trend, seasonality, and randomness.

#### 3.2. Exploring **Monthly Average for Every Year**

To delve deeper into my first look speculations, I also plotted a graph of monthly average of adjusted closing stock prices using the plot() function.

```{r}
# Extract month and year from the date
ap_price$month_year <- format(ap_price$date, "%Y-%m")

# Aggregate by month and calculate the mean
monthly_avg <- ap_price %>%
  group_by(month_year) %>%
  summarise(avg_adjusted = mean(ASIANPAINT.NS.Adjusted))

# Converting Month to Date Format
monthly_avg$month_year <- as.Date(paste0(monthly_avg$month_year, "-01"))

# Plot the line graph
plot(monthly_avg$month_year, monthly_avg$avg_adjusted,
     type = "l", col = "darkred", lwd = 2,
     main = "Monthly Average Adjusted Closing Price of Asian Paints",
     xlab = "Date", ylab = "Adjusted Closing Price")
```

**Inference:** This plot is somewhat similar to that of daily closing price, and also suggests that there is definitely some seasonality in the data.

#### 3.3. Aggregate Month-wise Average Stock Price

I also wanted to explore month-wise cyclicity regardless of the year, so I combined data of all the years and plotted a scatter plot of average monthly closing price.

```{r}
monthly_average <- ap_price %>%
  mutate(month = format(date, "%m")) %>%  # Extract month as a two-digit number
  group_by(month) %>%
  summarize(monthly_avg = mean(ASIANPAINT.NS.Adjusted, na.rm = TRUE))

# Convert month to numeric for plotting
monthly_average$month_num <- as.numeric(monthly_average$month)

# Plot the scatter plot
plot(monthly_average$month_num, monthly_average$monthly_avg,
     type = "p", pch = 19, col = "darkblue",
     xaxt = "n", xlab = "Month (April to January)", ylab = "Monthly Average Price",
     main = "Monthly Average Prices of Asian Paints")

# Add x-axis labels for reordered months
axis(1, at = 1:12, labels = levels(monthly_average$month))

```

#### 3.4. Month-wise Median and Variation in Stock Price

I also plotted a box-plot see the month-wise median and distribution of data.

```{r}

# Add month as a factor with desired levels to the ap_price dataset
ap_price <- ap_price %>%
  mutate(month = format(date, "%m"))  # Extract month as two-digit number

# Create the box plot
boxplot(ASIANPAINT.NS.Adjusted ~ month, data = ap_price,
        col = "lightblue", border = "darkblue",
        xlab = "Month (April to January)", ylab = "Adjusted Closing Price",
        main = "Monthly Price Distribution of Asian Paints",
        xaxt = "n")

# Add x-axis labels for reordered months
axis(1, at = 1:12, labels = levels(ap_price$month))

```

**Inference:** During the 2nd quarter of the financial year (July-Sept), Asian Paints' stock on average, experiences a higher closing price. Also, there seems a month-wise cyclicity in the closing stock prices. Also, Dec-Mar seems to more volatile than rest of the year.

#### 3.5. Finding the Sesonality Index in the Stock Prices - Central Moving Average Approach

I've used 12 month moving average to de-seasonalise the data, and divided stock prices with this to find the seasonality factor.

```{r}

# Ensure data is sorted by month_year
ap_price <- ap_price %>%
  arrange(month_year)

# Calculate monthly averages (if not already grouped by month)
monthly_data <- ap_price %>%
  group_by(month_year) %>%
  summarize(monthly_avg = mean(ASIANPAINT.NS.Adjusted, na.rm = TRUE)) %>%
  ungroup()

# Calculate 12-month trailing and central moving averages
monthly_data <- monthly_data %>%
  mutate(
    moving_avg_12 = rollmean(monthly_avg, k = 12, fill = NA, align = "center"),   # Trailing
  )

# Join the moving averages back to the original data
ap_price <- ap_price %>%
  left_join(monthly_data, by = "month_year")

# View the resulting data frame
head(ap_price)
```

```{r}

# Step 1: Calculate seasonality factor
monthly_data <- monthly_data %>%
  mutate(seasonality_factor = monthly_avg / moving_avg_12)

# Step 2: Extract month from month_year and calculate seasonality index
seasonality_index <- monthly_data %>%
  mutate(month = format(as.Date(paste0(month_year, "-01")), "%m")) %>%  # Extract month as "01", "02", etc.
  group_by(month) %>%
  summarize(seasonality_index = mean(seasonality_factor, na.rm = TRUE)) %>%
  ungroup()

# Step 3: Add seasonality index back to ap_price
ap_price <- ap_price %>%
  mutate(month = format(as.Date(paste0(month_year, "-01")), "%m")) %>%  # Extract month for matching
  left_join(seasonality_index, by = "month")

# View the resulting data frame
head(ap_price)
```

```{r}
# Calculate the sum of the seasonality_index column
total_seasonality_index <- sum(seasonality_index$seasonality_index, na.rm = TRUE)

# Print the result
print(total_seasonality_index)
```

**Inference:** Sum of seasonality index is fairly close to 12, given we have used only a 3-year data on for this. Seems like we might have captured some seasonality.

#### 3.6. Finding the Sesonality Index in the Stock Prices - Monthly Average Normalization Approach

I've normalized monthly average that we used to before to simply calculate the seasonality index.

```{r}

# Calculate the average of monthly_avg
avg_monthly_avg <- mean(monthly_average$monthly_avg, na.rm = TRUE)

# Calculate seasonality_index_2 for each month
monthly_average <- monthly_average %>%
  mutate(seasonality_index_2 = monthly_avg / avg_monthly_avg)

# Add seasonality_index_2 to ap_price based on the month
ap_price <- ap_price %>%
  mutate(month = format(date, "%m")) %>%  # Extract month as two-digit format
  left_join(monthly_average %>% select(month, seasonality_index_2), by = "month")

```

#### 3.7. Deseasonalizing the Prices

Using the seasonality indices we've just calculated, let's de-seasonalize our stock price data.

```{r}

# Ensure seasonality_index column is present and matches the structure of ap_price
ap_price <- ap_price %>%
  mutate(DS_Price = ASIANPAINT.NS.Adjusted / seasonality_index)

# Remove the moving_avg_12 column
ap_price <- ap_price %>%
  select(-moving_avg_12)
```

```{r}

# Ensure seasonality_index column is present and matches the structure of ap_price
ap_price <- ap_price %>%
  mutate(DS_Price_2 = ASIANPAINT.NS.Adjusted / seasonality_index_2)

# View the updated dataset
head(ap_price)
```

#### 3.8. Visualizing the de-seasonalized prices

**Approach-1**

```{r}

# Plot the DS_Price column as a line plot
plot(ap_price$date, ap_price$DS_Price,
     type = "l", col = "darkgreen", lwd = 1.5,
     main = "Deseasonalized Price of Asian Paints",
     xlab = "Date", ylab = "Deseasonalized Price")
```

```{r}
# Define a function to normalize dates by financial year
normalize_fy_dates <- function(df, start_date) {
  df %>%
    mutate(day_of_fy = as.numeric(date - as.Date(start_date) + 1)) %>%
    select(day_of_fy, DS_Price)
}

# Normalize dates for each financial year
ap_price_fy23 <- ap_price %>%
  filter(date >= as.Date("2022-04-01") & date <= as.Date("2023-03-31")) %>%
  normalize_fy_dates(start_date = "2022-04-01")

ap_price_fy24 <- ap_price %>%
  filter(date >= as.Date("2023-04-01") & date <= as.Date("2024-03-31")) %>%
  normalize_fy_dates(start_date = "2023-04-01")

ap_price_fy25 <- ap_price %>%
  filter(date >= as.Date("2024-04-01") & date <= as.Date("2025-03-31")) %>%
  normalize_fy_dates(start_date = "2024-04-01")

# Create the base plot for FY2023
plot(ap_price_fy23$day_of_fy, ap_price_fy23$DS_Price,
     type = "l", col = "blue", lwd = 1.5,
     main = "Deseasonalized Price of Asian Paints (FY23, FY24, FY25)",
     xlab = "Day of Financial Year", ylab = "Deseasonalized Price",
     ylim = range(c(ap_price_fy23$DS_Price, ap_price_fy24$DS_Price, ap_price_fy25$DS_Price), na.rm = TRUE))

# Add lines for FY2024 and FY2025
lines(ap_price_fy24$day_of_fy, ap_price_fy24$DS_Price, col = "red", lwd = 1.5)
lines(ap_price_fy25$day_of_fy, ap_price_fy25$DS_Price, col = "darkgreen", lwd = 1.5)

# Add a legend
legend("topright", legend = c("FY2023", "FY2024", "FY2025"),
       col = c("blue", "red", "darkgreen"), lty = 1, lwd = 1.5)

```

**Approach-2**

```{r}

# Plot the DS_Price_2 column as a line plot
plot(ap_price$date, ap_price$DS_Price_2,
     type = "l", col = "darkgreen", lwd = 1.5,
     main = "Deseasonalized Price of Asian Paints",
     xlab = "Date", ylab = "Deseasonalized Price")
```

```{r}
# Define a function to normalize dates by financial year
normalize_fy_dates <- function(df, start_date) {
  df %>%
    mutate(day_of_fy = as.numeric(date - as.Date(start_date) + 1)) %>%
    select(day_of_fy, DS_Price_2)
}

# Normalize dates for each financial year
ap_price_fy23 <- ap_price %>%
  filter(date >= as.Date("2022-04-01") & date <= as.Date("2023-03-31")) %>%
  normalize_fy_dates(start_date = "2022-04-01")

ap_price_fy24 <- ap_price %>%
  filter(date >= as.Date("2023-04-01") & date <= as.Date("2024-03-31")) %>%
  normalize_fy_dates(start_date = "2023-04-01")

ap_price_fy25 <- ap_price %>%
  filter(date >= as.Date("2024-04-01") & date <= as.Date("2025-03-31")) %>%
  normalize_fy_dates(start_date = "2024-04-01")
# Create the base plot for FY2023
plot(ap_price_fy23$day_of_fy, ap_price_fy23$DS_Price_2,
     type = "l", col = "blue", lwd = 1.5,
     main = "Deseasonalized Price of Asian Paints (FY23, FY24, FY25)",
     xlab = "Day of Financial Year", ylab = "Deseasonalized Price",
     ylim = range(c(ap_price_fy23$DS_Price_2, ap_price_fy24$DS_Price_2, ap_price_fy25$DS_Price_2), na.rm = TRUE))

# Add lines for FY2024 and FY2025
lines(ap_price_fy24$day_of_fy, ap_price_fy24$DS_Price_2, col = "red", lwd = 1.5)
lines(ap_price_fy25$day_of_fy, ap_price_fy25$DS_Price_2, col = "darkgreen", lwd = 1.5)

# Add a legend
legend("topright", legend = c("FY2023", "FY2024", "FY2025"),
       col = c("blue", "red", "darkgreen"), lty = 1, lwd = 1.5)

```

**Inference:** It seems we have removed some seasonality but there are still massive and seemingly random fluctuations in data. I prefer using the second set of seasonality indices, which seems slightly more effective.

```{r}
ap_price_xts = na.omit(ap_price$DS_Price_2)
```

### 4. Checking Stationarity in Daily Closing Price of Stock

#### 4.1. What is Stationarity?

Stationarity refers to a property of data where statistical properties such as mean, variance, and autocorrelation structure remain constant over time. In simpler terms, it means that the data does not exhibit trends, seasonality, or other systematic patterns that change over time.

#### 4.2. Augmented Dickey-Fuller (ADF) Test

Now, I'll perform ADF test to check whether the data is stationary or not.

Null Hypothesis - H0: Data is not stationary.\
Alternate Hypothesis - H1: Data is stationary.

```{r}
# Perform the Augmented Dickey-Fuller (ADF) test
adf_result <- adf.test(ap_price_xts)

# Print the ADF test results
print(adf_result)
```

As p \> 5%\
So, Null Hypothesis can't be Rejected.

**Inference:** Time series of de-seasonalised closing price of Asian paints is Non-Stationary, i.e. it may have some trend or seasonality.

### 5. Checking Stationarity in Daily Logarithmic Returns of Stock

#### 5.1. Exploring Daily Logarithmic Returns of Asian Paints

As the closing price does not have stationarity, I tried to explore the first order difference, i.e. the logarithm of difference of daily closing prices.

```{r}
ap_log_returns = na.omit(diff(log(ap_price_xts)))
#View(ap_ds)
plot(index(ap_log_returns), ap_log_returns,
     type = "l",
     lwd = 1,
     main = "Daily Logarithmic Returns of Asian Paints",
     xlab = "Year", ylab = "")
```

**Inference:** The trend in data is no longer visible, and it seems that data might have become stationary.

#### 5.2. Augmented Dickey-Fuller (ADF) Test

Now, lets check whether the daily log returns passes the stationarity test or not.

```{r}
# Perform the Augmented Dickey-Fuller (ADF) test
adf_result_2 <- adf.test(ap_log_returns)

# Print the ADF test results
print(adf_result_2)
```

As p \< 5%\
Null Hypothesis is Rejected.

**Inference:** Daily Logarithmic Returns of Asian Paints are Stationary.

### 6. Checking for Autocorrelation

#### **6.1. What is Auto-correlation?**

Autocorrelation, also known as serial correlation, is a statistical phenomenon that measures the degree of linear relationship between consecutive observations in a time series data. Specifically, autocorrelation measures the correlation between an observation and its past observations at various lags (time intervals).

#### **6.2. Ljung-Box or Box-Pierce Test**

Now, we've established stationarity, let's check for Autocorrelation in daily log returns using the box test.

Null Hypothesis - H0: No Autocorrelation\
Alternate Hypothesis - H1: Has Autocorrelation

```{r}
lb_result = Box.test(ap_log_returns, lag=10)
print(lb_result)
```

As p \> 5%\
So, Null Hypothesis can't be Rejected.

**Inference:** We may assume that there is no significant autocorrelation in log returns of Asian Paints, but let's probe it further.

### 7. ARIMA Modelling for Autocorrelation

#### 7.1. What is ARIMA?

An Auto-Regressive Integrated Moving Average, or ARIMA, is a statistical analysis model that uses time series data to better understand the data and to predict future trends. It has following 3 components:

-   **Autoregression (AR):** It refers to a model that shows a changing variable that regresses on its own lagged, or prior, values.

-   **Integrated (I):** represents the differencing of raw observations to allow the time series to become stationary (i.e., data values are replaced by the difference between the data values and the previous values).

-   **Moving average (MA):** incorporates the dependency between an observation and a residual error from a moving average model applied to lagged observations.

#### 7.2. What is an Autocorrelation Function (ACF)?

The Autocorrelation Function (ACF) is a statistical tool used to measure the correlation between observations in a time series at different time lags. It quantifies the relationship between an observation and its past observations.

#### **7.3. Autocorrelation Function (ACF) Of Logarithm of Daily Return**

Now, to develop an appropriate ARIMA model, I tried to explore the autocorrelation function for daily log return using acf() function.

```{r}
acf(ap_log_returns)
```

#### 7.4. What is a Partial Autocorrelation Function (PACF)?

The Partial Autocorrelation Function (PACF) is a statistical tool used in time series analysis to measure the correlation between observations in a time series at different lags, while controlling for the intermediate lags. In other words, it quantifies the direct relationship between an observation and its past observations, excluding the indirect effects of the intermediate observations.

#### 7.5. Partial Autocorrelation Function (PACF) of Logarithm of Daily Return

I also ran pacf() function to look at the partial autocorrelations at different time lags.

```{r}
pacf(ap_log_returns)
```

**Inference:** It indicates some correlations at lag=25, might be because a typical month at stock market is also of nearly 25 days. Let's try a model with AR=6, MA=3, based on spikes on the curves.

#### **7.6. ARIMA MODEL 1 - WITH ORDER(6,0,3) - My Suggested Model**

Now, I'm creating an ARIMA Model manually using the arima() function with the order I feel fits the best by looking at the acf() and pacf() functions. I named this model as **"arima_model1"**.

```{r}
arima_model1 = arima(ap_log_returns, order = c(6,0,3))

# 1. Calculate Residuals
arima1_residuals <- residuals(arima_model1)

# 2. Compute Metrics

# Root Mean Squared Error (RMSE)
rmse1 <- sqrt(mean(arima1_residuals^2))

# Mean Absolute Percentage Error (MAPE)
mape1 <- mean(abs(arima1_residuals / ap_log_returns), na.rm = TRUE) * 100

# Adjusted R-Squared
# Total sum of squares
sst <- sum((ap_log_returns - mean(ap_log_returns, na.rm = TRUE))^2, na.rm = TRUE)
# Residual sum of squares
ssr <- sum(arima1_residuals^2, na.rm = TRUE)
# Adjusted R-Squared formula
n <- length(ap_log_returns)  # Number of observations
p <- length(coef(arima_model1))  # Number of parameters
adjusted_r_squared_1 <- 1 - ((1 - (1 - (ssr / sst))) * ((n - 1) / (n - p - 1)))

# 3. Print Results
cat("Metrics for ARIMA Model:\n")
cat(sprintf("RMSE: %.4f\n", rmse1))
cat(sprintf("MAPE: %.2f%%\n", mape1))
cat(sprintf("Log-Likelihood: %.4f\n", logLik(arima_model1)))
cat(sprintf("AIC: %.4f\n", AIC(arima_model1)))
cat(sprintf("BIC: %.4f\n", BIC(arima_model1)))
cat(sprintf("Adjusted R-Squared: %.4f\n", adjusted_r_squared_1))
```

#### **7.7. ARIMA MODEL 2 - WITH ORDER(0,0,0) - Generated by Auto ARIMA**

I also ran auto.arima() function to have another model in case my model fails. This model came out to be of order (0,0,0) i.e. the original values and I named it as **"arima_model2"**.

```{r}
arima_model2 = auto.arima(ap_log_returns)

# 1. Calculate Residuals
arima2_residuals <- residuals(arima_model2)

# 2. Compute Metrics

# Root Mean Squared Error (RMSE)
rmse2 <- sqrt(mean(arima2_residuals^2))

# Mean Absolute Percentage Error (MAPE)
mape2 <- mean(abs(arima2_residuals / ap_log_returns), na.rm = TRUE) * 100

# Adjusted R-Squared
# Total sum of squares
sst <- sum((ap_log_returns - mean(ap_log_returns, na.rm = TRUE))^2, na.rm = TRUE)
# Residual sum of squares
ssr <- sum(arima2_residuals^2, na.rm = TRUE)
# Adjusted R-Squared formula
n <- length(ap_log_returns)  # Number of observations
p <- length(coef(arima_model2))  # Number of parameters
adjusted_r_squared_2 <- 1 - ((1 - (1 - (ssr / sst))) * ((n - 1) / (n - p - 1)))

# 3. Print Results
cat("Metrics for ARIMA Model:\n")
cat(sprintf("RMSE: %.4f\n", rmse2))
cat(sprintf("MAPE: %.2f%%\n", mape2))
cat(sprintf("Log-Likelihood: %.4f\n", logLik(arima_model2)))
cat(sprintf("AIC: %.4f\n", AIC(arima_model2)))
cat(sprintf("BIC: %.4f\n", BIC(arima_model2)))
cat(sprintf("Adjusted R-Squared: %.4f\n", adjusted_r_squared_2))
```

#### **7.8. ARIMA MODEL 3 - WITH ORDER(1,0,0) - Another Suggested Model**

```{r}
arima_model3 = arima(ap_log_returns, order = c(1,0,0))

# 1. Calculate Residuals
arima3_residuals <- residuals(arima_model3)

# 2. Compute Metrics

# Root Mean Squared Error (RMSE)
rmse3 <- sqrt(mean(arima3_residuals^2))

# Mean Absolute Percentage Error (MAPE)
mape3 <- mean(abs(arima3_residuals / ap_log_returns), na.rm = TRUE) * 100

# Adjusted R-Squared
# Total sum of squares
sst <- sum((ap_log_returns - mean(ap_log_returns, na.rm = TRUE))^2, na.rm = TRUE)
# Residual sum of squares
ssr <- sum(arima3_residuals^2, na.rm = TRUE)
# Adjusted R-Squared formula
n <- length(ap_log_returns)  # Number of observations
p <- length(coef(arima_model3))  # Number of parameters
adjusted_r_squared_3 <- 1 - ((1 - (1 - (ssr / sst))) * ((n - 1) / (n - p - 1)))

# 3. Print Results
cat("Metrics for ARIMA Model:\n")
cat(sprintf("RMSE: %.4f\n", rmse3))
cat(sprintf("MAPE: %.2f%%\n", mape3))
cat(sprintf("Log-Likelihood: %.4f\n", logLik(arima_model3)))
cat(sprintf("AIC: %.4f\n", AIC(arima_model3)))
cat(sprintf("BIC: %.4f\n", BIC(arima_model3)))
cat(sprintf("Adjusted R-Squared: %.4f\n", adjusted_r_squared_3))
```

#### **7.9. ARIMA MODEL 4 - WITH ORDER(0,0,1) - Another Suggested Model**

```{r}
arima_model4 = arima(ap_log_returns, order = c(0,0,1))

# 1. Calculate Residuals
arima4_residuals <- residuals(arima_model4)

# 2. Compute Metrics

# Root Mean Squared Error (RMSE)
rmse4 <- sqrt(mean(arima4_residuals^2))

# Mean Absolute Percentage Error (MAPE)
mape4 <- mean(abs(arima4_residuals / ap_log_returns), na.rm = TRUE) * 100

# Adjusted R-Squared
# Total sum of squares
sst <- sum((ap_log_returns - mean(ap_log_returns, na.rm = TRUE))^2, na.rm = TRUE)
# Residual sum of squares
ssr <- sum(arima4_residuals^2, na.rm = TRUE)
# Adjusted R-Squared formula
n <- length(ap_log_returns)  # Number of observations
p <- length(coef(arima_model4))  # Number of parameters
adjusted_r_squared_4 <- 1 - ((1 - (1 - (ssr / sst))) * ((n - 1) / (n - p - 1)))

# 3. Print Results
cat("Metrics for ARIMA Model:\n")
cat(sprintf("RMSE: %.4f\n", rmse4))
cat(sprintf("MAPE: %.2f%%\n", mape4))
cat(sprintf("Log-Likelihood: %.4f\n", logLik(arima_model4)))
cat(sprintf("AIC: %.4f\n", AIC(arima_model4)))
cat(sprintf("BIC: %.4f\n", BIC(arima_model4)))
cat(sprintf("Adjusted R-Squared: %.4f\n", adjusted_r_squared_4))
```

**Inference:** All the models are quite unfit, also ljung-box test suggests no significant correlation, so its better not to use any ARIMA model.

### 8. Exploring Logarithmic Daily Return of the Stock

#### **8.1. Average Logarithmic Daily Return of Asian Paints**

Now, that I'm working with Log of Daily Returns of a stock, and built ARIMA Models for it, its better to delve deeper and understand this data better. So first , let's look at the mean of our data.

```{r}
mean_ap_log_return = mean(ap_log_returns)
print(paste0("Average Logarithmic Daily Return of Asian Paints:", round(mean_ap_log_return,6)))
```

#### **8.2. Plotting Variances of Log Return**

Next up, let's look at how the data varies over time by plotting variances of the data from its mean.

```{r}
ap_log_returns_sq = (ap_log_returns-mean_ap_log_return)^2
plot(index(ap_log_returns_sq), ap_log_returns_sq,
     type = "l",
     lwd = 1,
     main = "Variances of Logarithmic Daily Return of Asian Paints",
     xlab = "Year", ylab = "")
```

#### **8.3. Plotting Residuals of Log Return**

```{r}
mean_model_residuals = (ap_log_returns-mean_ap_log_return)
plot(index(mean_model_residuals), ap_log_returns_sq,
     type = "l",
     lwd = 1,
     main = "Residuals of Logarithmic Daily Return of Asian Paints",
     xlab = "Year", ylab = "")
```

The data seems to have a good amount of variation in its deviation from mean, also volatility seems to cluster, which indicates heteroscedasticity.

**Inference:** The data might have heteroscedasticity.

#### **8.3. Overall Variation in of Log Returns**

Now, let's look at the variation in our data by calculating its variance, standard deviation, and coefficient of variation.

```{r}
ap_log_return_variance = mean(ap_log_returns_sq)
print(paste0("Variance of Log Return of Asian Paints: ", round(ap_log_return_variance,6)))
ap_log_return_sd = sqrt(ap_log_return_variance)
print(paste0("Standard Deviation of Log Return of Asian Paints: ", round(ap_log_return_sd,6)))
print(paste0("Coefficient of Variation of Log Return of Asian Paints: ", round(ap_log_return_sd/mean_ap_log_return,4)))
```

### 9. Checking for Heteroskedasticity for Log of Daily Returns

#### 9.1. What is Heteroscedasticity?

Heteroscedasticity is a situation where the variability of a variable (often the residuals or errors in a regression model) is not constant across all levels of another variable. In simpler terms, it means that the spread of the data points a.k.a. residuals changes as the value of another variable changes.

#### **9.2. Box Test for Heteroscedasticity**

Null Hypothesis - H0: There is No Heteroscedasticity in the residuals\
Alternate Hypothesis - H1: There is Heteroscedasticity in the residuals

```{r}
print(Box.test(ap_log_returns_sq, lag = 1))
```

```{r}
print(Box.test(ap_log_returns_sq, lag = 2))
```

```{r}
print(Box.test(ap_log_returns_sq, lag = 10))
```

```{r}
print(Box.test(ap_log_returns_sq, lag = 25))
```

#### **9.3. ARCH LM Test for Heteroscedasticity**

Null Hypothesis - H0: There is No Heteroscedasticity in the residuals\
Alternate Hypothesis - H1: There is Heteroscedasticity in the residuals

```{r}
print(ArchTest(arima_model2$residuals, lags = 1))
```

```{r}
print(ArchTest(arima_model2$residuals, lags = 2))
```

```{r}
print(ArchTest(arima_model2$residuals, lags = 10))
```

```{r}
print(ArchTest(arima_model2$residuals, lags = 25))
```

As p \> 5%\
Null Hypothesis can't be Rejected.

**Inference:** We're safe to assume that there is no significant heteroscedasticity in the series based on these tests. But let's try to model it if we can get a better model.

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-- GARCH Model Needs to be Updated -------------------------------------

### 11. Capturing Volatility with GARCH Models

#### **11.1. What is GARCH Model?**

The GARCH (Generalized Auto-Regressive Conditional Heteroscedasticity) model is a statistical model used to capture the time-varying volatility or variance clustering observed in financial time series data. It extends the ARCH model by incorporating not only past squared residuals but also past volatility values to model the conditional variance of the data.

#### 11.2. Standard GARCH Model with Constant Mean and AR Order 0

Now, by assuming GARCH order to be 1, and AR order 0 which we speculated earlier by looking at acf() and pacf() curves, we built a model named **"garch_model1"**.

```{r}
garch_model1 = ugarchspec(variance.model = list(model = 'sGARCH', garchOrder = c(1,1)), mean.model = list(armaOrder = c(0,0), include.mean = TRUE))
ap_log_returns_garch1 = ugarchfit(garch_model1, data = ap_log_returns_sq)
print(ap_log_returns_garch1)
```

#### **11.3. Simple GARCH Model with Constant Mean and AR Order 1**

Now, by assuming GARCH order to be 1, and AR order 1 which we received from auto arima, we built a model named **"garch_model2"**.

```{r}
garch_model2 = ugarchspec(variance.model = list(model = 'sGARCH', garchOrder = c(1,1)), mean.model = list(armaOrder = c(1,0), include.mean = TRUE))
ap_log_returns_garch2 = ugarchfit(garch_model2, data = ap_log_returns_sq)
print(ap_log_returns_garch2)
```

#### **11.4. ARCH LM Test for Heteroscedasticity - GARCH Model 1**

Null Hypothesis - H0: There is No Heteroscedasticity in the residuals\
Alternate Hypothesis - H1: There is Heteroscedasticity in the residuals

```{r}
# Perform ARCH LM test
arch_test_model1 <- ArchTest(residuals(ap_log_returns_garch1), lags = 20)
print(arch_test_model1)
```

As p = 0.1255 i.e. p \> 5%\
Null Hypothesis cannot be Rejected.

**Inference:** Residuals of GARCH Model 1 may not have conditional heteroscedasticity.

#### **11.5. ARCH LM Test for Heteroscedasticity - GARCH Model 2**

Null Hypothesis - H0: There is No Heteroscedasticity in the residuals\
Alternate Hypothesis - H1: There is Heteroscedasticity in the residuals

```{r}
# Perform ARCH LM test
arch_test_model2 <- ArchTest(residuals(ap_log_returns_garch2), lags = 20)
print(arch_test_model2)
```

As p = 5.6e-07 i.e. p \< 5%\
Null Hypothesis is Rejected.

**Inference:** Residuals of GARCH Model 2 have conditional heteroscedasticity.

#### **11.6. Visualizing Residuals - GARCH Model 1**

Though GARCH Model 1 passed the heteroscedasticity test, but let's visualize its residuals to probe further.

```{r}
garch_residuals = (residuals(ap_log_returns_garch1))
plot(index(garch_residuals), garch_residuals,
     type = "l",
     lwd = 1,
     main = "Residuals of GARCH Model 1",
     xlab = "Year", ylab = "")
```

**Inference:** Extraordinarily High Volatility in 2020 due to Covid-19 can be Observed.

### 12. Using GARCH Model 1 for Forecasting

```{r}
garch_forecast = ugarchforecast(ap_log_returns_garch1, n.ahead = 100)
garch_forecast
```

```{r}
plot(garch_forecast)
```
